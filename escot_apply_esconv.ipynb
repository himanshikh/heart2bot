{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjDp5fA_jJCG",
        "outputId": "7bb01f02-3a0a-444d-df8a-6bf236ce7ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/ESCoT\n",
            "Creating required directories...\n",
            "Directories created/verified.\n",
            "Installing dependencies...\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: datasets>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.28.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.57.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.23.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.48.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.17.0->-r requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 13)) (2.40.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (3.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13)) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=1.17.0->-r requirements.txt (line 2)) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 13)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 13)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.17.0->-r requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.17.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.17.0->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.17.0->-r requirements.txt (line 2)) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.17.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.17.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.17.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.17.0->-r requirements.txt (line 2)) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.17.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Downloading NLTK punkt resource...\n",
            "NLTK punkt found.\n",
            "\n",
            "Logging into Hugging Face...\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup, Installation, File Creation, and Hugging Face Login\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- Clone Repository and Initial Setup ---\n",
        "if not os.path.exists(\"ESCoT\"):\n",
        "    print(\"Cloning ESCoT repository...\")\n",
        "    !git clone https://github.com/TeigenZhang/ESCoT.git\n",
        "os.chdir(\"ESCoT\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- Define Paths and Create Directories Defensively ---\n",
        "BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "OUTPUT_DIR = \"./ESCOT_7B_Checkpoint\"\n",
        "EVAL_OUTPUT_DIR = \"./ESCOT_EVAL_RESULTS\"\n",
        "\n",
        "print(\"Creating required directories...\")\n",
        "os.makedirs(\"utils\", exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)\n",
        "print(\"Directories created/verified.\")\n",
        "\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "print(\"Installing dependencies...\")\n",
        "# Install all required libraries including trl, bitsandbytes, and evaluation tools\n",
        "!pip install -r requirements.txt\n",
        "!pip install accelerate bitsandbytes peft trl transformers torch==2.3.0 rouge-score nltk\n",
        "# Download NLTK data for evaluation\n",
        "import nltk\n",
        "print(\"Downloading NLTK punkt resource...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK punkt found.\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    print(\"NLTK punkt downloaded.\")\n",
        "\n",
        "\n",
        "# --- HUGGING FACE LOGIN ---\n",
        "from huggingface_hub import login\n",
        "print(\"\\nLogging into Hugging Face...\")\n",
        "# ⚠️ ACTION REQUIRED: Replace your token here\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- A. Create utils/merge.py (Merge Utility) ---\n",
        "MERGE_PY_CONTENT = \"\"\"\n",
        "import os\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    LlamaTokenizer\n",
        ")\n",
        "\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "\n",
        "def merge_llm_with_lora(base_model_name, adapter_model_name, output_name, push_to_hub=False):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_model_name)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    if \"decapoda\" in base_model_name.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(base_model_name)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "                \"pad_token\": DEFAULT_PAD_TOKEN,\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
        "\n",
        "    if push_to_hub:\n",
        "        print(f\"Saving to hub ...\")\n",
        "        model.push_to_hub(f\"{base_model_name}-merged\", use_temp_dir=False, private=True)\n",
        "        tokenizer.push_to_hub(f\"{base_model_name}-merged\", use_temp_dir=False, private=True)\n",
        "    else:\n",
        "        output_name = os.path.join(output_name, \"final_checkpoint-merged\")\n",
        "        model.save_pretrained(output_name)\n",
        "        tokenizer.save_pretrained(output_name)\n",
        "        print(f\"Model saved to {output_name}\")\n",
        "\"\"\"\n",
        "with open(\"utils/merge.py\", \"w\") as f:\n",
        "    f.write(MERGE_PY_CONTENT)\n",
        "print(\"Created utils/merge.py successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE-6nQu-ldcP",
        "outputId": "12662665-ac9b-4c68-8c59-40751e3f629d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created utils/merge.py successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- B. Create supervised_finetune_llama2_cot.py (Training Script) ---\n",
        "# NOTE: The content here remains the same as it was already correct.\n",
        "SFT_PY_CONTENT = \"\"\"\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    LlamaTokenizer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "from utils.merge import merge_llm_with_lora\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--base_model\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--dataset_name\", type=str, default=\"./data/ablation_data/em_es_ia_sr_re\") # Defaulting to your full dataset path\n",
        "    parser.add_argument(\"--split\", type=str, default=\"train\")\n",
        "    parser.add_argument(\"--size_valid_set\", type=int, default=4000)\n",
        "    parser.add_argument(\"--streaming\", action=\"store_true\", default=False)\n",
        "    parser.add_argument(\"--shuffle_buffer\", type=int, default=5000)\n",
        "\n",
        "    parser.add_argument(\"--seq_length\", type=int, default=2048) # Your provided value\n",
        "    parser.add_argument(\"--max_steps\", type=int, default=10000) # Your provided value\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8) # Your provided value\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "    parser.add_argument(\"--eos_token_id\", type=int, default=2)\n",
        "\n",
        "    parser.add_argument(\"--lora_r\", type=int, default=16)\n",
        "    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
        "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
        "    parser.add_argument(\"--lora_target_modules\", type=str, default=\"q_proj,v_proj,o_proj,k_proj,gate_proj,up_proj,down_proj\") # Full LLaMA targets\n",
        "\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-5) # Your provided value\n",
        "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\") # Your provided value\n",
        "    parser.add_argument(\"--num_warmup_steps\", type=int, default=100)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
        "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.)\n",
        "\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
        "    parser.add_argument(\"--fp16\", action=\"store_true\", default=True)\n",
        "    parser.add_argument(\"--no_bf16\", action=\"store_false\", default=True)\n",
        "    parser.add_argument(\"--no_gradient_checkpointing\", action=\"store_false\", default=True)\n",
        "    parser.add_argument(\"--seed\", type=int, default=1104) # Your provided value\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=None)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints/cot/supervised_llama2_cot\") # Your provided path\n",
        "    parser.add_argument(\"--log_freq\", type=int, default=1)\n",
        "    parser.add_argument(\"--eval_freq\", type=int, default=500) # Your provided value\n",
        "    parser.add_argument(\"--save_freq\", type=int, default=500) # Your provided value\n",
        "    parser.add_argument(\"--save_total_limit\", type=int, default=100) # Your provided value\n",
        "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n",
        "    parser.add_argument(\"--run_name\", type=str, default=\"supervised_llama2_cot\") # Your provided value\n",
        "    parser.add_argument(\"--merge_lora\", action=\"store_true\", default=True)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    max_token_length = 0\n",
        "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
        "        text = prepare_sample_text(example)\n",
        "        total_characters += len(text)\n",
        "        if tokenizer.is_fast:\n",
        "            total_tokens += len(tokenizer(text).tokens())\n",
        "            if len(tokenizer(text).tokens()) > max_token_length:\n",
        "                max_token_length = len(tokenizer(text).tokens())\n",
        "        else:\n",
        "            total_tokens += len(tokenizer.tokenize(text))\n",
        "            if len(tokenizer.tokenize(text)) > max_token_length:\n",
        "                max_token_length = len(tokenizer.tokenize(text))\n",
        "\n",
        "    print(f\"max token length: {max_token_length}\")\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def prepare_sample_text(data_point):\n",
        "    if data_point[\"input\"]:\n",
        "        return f\\\"\\\"\\\"<s>Human:\n",
        "{data_point[\"input\"]}\n",
        "{data_point[\"instruction\"]}\n",
        "</s><s>Assistant:\n",
        "{data_point[\"output\"]}\n",
        "</s>\\\"\\\"\\\"\n",
        "    else:\n",
        "        return f\\\"\\\"\\\"<s>Human:\n",
        "{data_point[\"instruction\"]}\n",
        "</s><s>Assistant:\n",
        "{data_point[\"output\"]}\n",
        "</s>\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "def create_datasets(tokenizer, args):\n",
        "    # This assumes the dataset_name points to a directory structure like:\n",
        "    # ./data/ablation_data/em_es_ia_sr_re/empathetic_dialogue_train.json\n",
        "    train_json_path = os.path.join(args.dataset_name, \"empathetic_dialogue_train.json\")\n",
        "    train_data = load_dataset(\"json\", data_files=train_json_path, split=\"train\")\n",
        "    train_data = train_data.shuffle(seed=args.seed)\n",
        "\n",
        "    # Note: Using the train.json path provided in the ablation script as the base path.\n",
        "    val_json_path = os.path.join(os.path.dirname(args.dataset_name), \"empathetic_dialogue_valid.json\")\n",
        "    valid_data = load_dataset(\"json\", data_files=val_json_path, split=\"train\")\n",
        "    valid_data = valid_data.shuffle(seed=args.seed)\n",
        "\n",
        "    chars_per_token = 3.6\n",
        "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        formatting_func=prepare_sample_text,\n",
        "        infinite=True,\n",
        "        seq_length=args.seq_length,\n",
        "        chars_per_token=chars_per_token,\n",
        "    )\n",
        "    valid_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        valid_data,\n",
        "        formatting_func=prepare_sample_text,\n",
        "        infinite=False,\n",
        "        seq_length=args.seq_length,\n",
        "        chars_per_token=chars_per_token,\n",
        "    )\n",
        "\n",
        "    print(f\"Size of the train dataset: {len(train_dataset)}\")\n",
        "    print(f\"Size of the validation dataset: {len(valid_dataset)}\")\n",
        "\n",
        "    return train_dataset, valid_dataset\n",
        "\n",
        "\n",
        "def run_training(args, train_data, val_data, tokenizer=None):\n",
        "    print(\"Loading the model\")\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        target_modules=args.lora_target_modules.split(','),\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    train_data.start_iteration = 0\n",
        "\n",
        "    print(\"Starting main loop\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        dataloader_drop_last=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        max_steps=args.max_steps,\n",
        "        eval_steps=args.eval_freq,\n",
        "        save_steps=args.save_freq,\n",
        "        logging_steps=args.log_freq,\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        learning_rate=args.learning_rate,\n",
        "        lr_scheduler_type=args.lr_scheduler_type,\n",
        "        warmup_steps=args.num_warmup_steps,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        gradient_checkpointing=args.no_gradient_checkpointing,\n",
        "        fp16=args.fp16,\n",
        "        bf16=args.no_bf16,\n",
        "        weight_decay=args.weight_decay,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        run_name=args.run_name,\n",
        "        report_to=\"wandb\",\n",
        "        ddp_find_unused_parameters=False if int(os.environ.get(\"WORLD_SIZE\", 1)) != 1 else None,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.base_model,\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "\n",
        "    if args.resume_from_checkpoint:\n",
        "        import torch\n",
        "        from peft import (\n",
        "            get_peft_model,\n",
        "            prepare_model_for_int8_training,\n",
        "            set_peft_model_state_dict\n",
        "        )\n",
        "        checkpoint_name = os.path.join(\n",
        "            args.resume_from_checkpoint, \"pytorch_model.bin\"\n",
        "        )\n",
        "        if not os.path.exists(checkpoint_name):\n",
        "            checkpoint_name = os.path.join(\n",
        "                args.resume_from_checkpoint, \"adapter_model.bin\"\n",
        "            )\n",
        "            args.resume_from_checkpoint = None\n",
        "\n",
        "        if os.path.exists(checkpoint_name):\n",
        "            print(f\"Restarting from {checkpoint_name}\")\n",
        "            model = prepare_model_for_int8_training(model)\n",
        "            model = get_peft_model(model, lora_config)\n",
        "\n",
        "            adapters_weights = torch.load(checkpoint_name)\n",
        "            set_peft_model_state_dict(model, adapters_weights)\n",
        "        else:\n",
        "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        peft_config=lora_config,\n",
        "        max_seq_length=args.seq_length,\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    print(\"Training...\")\n",
        "    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
        "\n",
        "    print(\"Saving last checkpoint of the model\")\n",
        "    final_model_path = os.path.join(args.output_dir, \"final_checkpoint/\")\n",
        "    trainer.model.save_pretrained(final_model_path)\n",
        "\n",
        "    if args.merge_lora:\n",
        "        merge_llm_with_lora(args.base_model, final_model_path, args.output_dir)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    if \"llama\" in args.base_model.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(args.base_model)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": \"</s>\",\n",
        "                \"bos_token\": \"</s>\",\n",
        "                \"unk_token\": \"</s>\",\n",
        "                \"pad_token\": \"</s>\",\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=False)\n",
        "        if getattr(tokenizer, \"pad_token\", None) is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    train_dataset, eval_dataset = create_datasets(tokenizer, args)\n",
        "    run_training(args, train_dataset, eval_dataset, tokenizer)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    assert args.base_model != \"\", \"Please provide the llama model path\"\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    logging.set_verbosity_error()\n",
        "\n",
        "    main(args)\n",
        "\"\"\"\n",
        "with open(\"supervised_finetune_llama2_cot.py\", \"w\") as f:\n",
        "    f.write(SFT_PY_CONTENT)\n",
        "print(\"Created supervised_finetune_llama2_cot.py successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjtzMkY2lrPF",
        "outputId": "68c4e834-8209-450c-fcfb-16a611f69982"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created supervised_finetune_llama2_cot.py successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- C. Create eval_script.py (Evaluation Script) ---\n",
        "EVAL_SCRIPT_CONTENT = \"\"\"\n",
        "import json\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import argparse\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "\n",
        "# --- Utility Function ---\n",
        "def prepare_sample_text(data_point):\n",
        "    if data_point[\"input\"]:\n",
        "        return f\\\"\\\"\\\"<s>Human:\n",
        "{data_point[\"input\"]}\n",
        "{data_point[\"instruction\"]}\n",
        "</s><s>Assistant:\\\"\\\"\\\"\n",
        "    else:\n",
        "        return f\\\"\\\"\\\"<s>Human:\n",
        "{data_point[\"instruction\"]}\n",
        "</s><s>Assistant:\\\"\\\"\\\"\n",
        "\n",
        "# --- Metric Function ---\n",
        "def calculate_metrics(results):\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l_scores = []\n",
        "    strategy_matches = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    strategy_pattern = re.compile(r'(?:strategy|recognized_strategy|Strategy):\\s*(\\w[\\w\\s]*?)(?:\\.|\\n|$)', re.IGNORECASE)\n",
        "\n",
        "    for item in results:\n",
        "        score = scorer.score(item['label'], item['prediction'])\n",
        "        rouge_l_scores.append(score['rougeL'].fmeasure)\n",
        "\n",
        "        total_samples += 1\n",
        "\n",
        "        pred_match = strategy_pattern.search(item['prediction'])\n",
        "        label_match = strategy_pattern.search(item['label'])\n",
        "\n",
        "        if pred_match and label_match:\n",
        "            pred_strategy = pred_match.group(1).strip()\n",
        "            label_strategy = label_match.group(1).strip()\n",
        "\n",
        "            if pred_strategy.lower() == label_strategy.lower():\n",
        "                strategy_matches += 1\n",
        "\n",
        "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0\n",
        "    strategy_accuracy = strategy_matches / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        \"ROUGE-L_F1\": avg_rouge_l,\n",
        "        \"Strategy_Accuracy\": strategy_accuracy,\n",
        "        \"Total_Samples\": total_samples\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "def main(args):\n",
        "    device = torch.device(f\"cuda:{args.gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    print(f\"Loading model from {args.model_path}...\")\n",
        "    model = LlamaForCausalLM.from_pretrained(args.model_path, device_map=device, low_cpu_mem_usage=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
        "\n",
        "    with open(args.json_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for data_point in tqdm(data):\n",
        "        prepared_text = prepare_sample_text(data_point)\n",
        "        input_ids = tokenizer(prepared_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        prepared_text_decoded = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        outputs = model.generate(input_ids, max_new_tokens=500, do_sample=True, top_k=30, top_p=0.85, temperature=0.5, repetition_penalty=1., eos_token_id=2, bos_token_id=1, pad_token_id=0)\n",
        "\n",
        "        rets = tokenizer.batch_decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "\n",
        "        full_output = rets[0].strip()\n",
        "        response = full_output.replace(prepared_text_decoded, \"\").strip()\n",
        "\n",
        "        response = response.replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        results.append({'input': data_point['input'], 'label': data_point['output'], 'prediction': response, 'dialog_id': data_point.get('dialog_id', 'N/A')})\n",
        "\n",
        "    # --- 1. Save results to file ---\n",
        "    output_filename = 'test_inference_results.json'\n",
        "    output_filepath = os.path.join(args.output_dir, output_filename)\n",
        "\n",
        "    with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
        "    print(f\"\\nInference results saved to: {output_filepath}\")\n",
        "\n",
        "    # --- 2. Calculate and Save Metrics ---\n",
        "    print(\"\\n--- Calculating Metrics ---\")\n",
        "    final_metrics = calculate_metrics(results)\n",
        "\n",
        "    metrics_filepath = os.path.join(args.output_dir, 'evaluation_metrics.json')\n",
        "    with open(metrics_filepath, 'w') as metric_file:\n",
        "        json.dump(final_metrics, metric_file, indent=4)\n",
        "\n",
        "    print(f\"Metrics saved to: {metrics_filepath}\")\n",
        "\n",
        "    print(\"\\n--- ESCoT BASELINE RESULTS ---\")\n",
        "    print(json.dumps(final_metrics, indent=4))\n",
        "\n",
        "# --- Argument Parsing ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to the ESCoT fine-tuned checkpoint.\")\n",
        "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"The id of the GPU to be used.\")\n",
        "    # Assuming test.json is the target for final evaluation\n",
        "    parser.add_argument(\"--json_path\", type=str, default=\"data/test.json\", help=\"Path to the JSON file containing the data.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"evaluation_output\", help=\"Directory to save results.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "    main(args)\n",
        "\"\"\"\n",
        "with open(\"eval_script.py\", \"w\") as f:\n",
        "    f.write(EVAL_SCRIPT_CONTENT)\n",
        "print(\"Created eval_script.py successfully.\")\n",
        "\n",
        "print(\"\\nAll files created and setup complete. \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWdpEnvbl4eB",
        "outputId": "50d2f5de-779e-496c-e5d6-3ec0f0a94934"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created eval_script.py successfully.\n",
            "\n",
            "All files created and setup complete. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-1820884398.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  strategy_pattern = re.compile(r'(?:strategy|recognized_strategy|Strategy):\\s*(\\w[\\w\\s]*?)(?:\\.|\\n|$)', re.IGNORECASE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Execute Training (ESCOT Fine-Tuning)\n",
        "# NOTE: Set --max_steps to 10000 for your final baseline run!\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "OUTPUT_DIR = \"./checkpoints/cot/supervised_llama2_cot\"\n",
        "\n",
        "print(f\"Starting training on {BASE_MODEL}. Checkpoint will be saved to {OUTPUT_DIR}/final_checkpoint-merged\")\n",
        "\n",
        "!accelerate launch supervised_finetune_llama2_cot.py \\\n",
        "    --base_model {BASE_MODEL} \\\n",
        "    --output_dir {OUTPUT_DIR} \\\n",
        "    --max_steps 500 \\\n",
        "    --save_freq 100 \\\n",
        "    --eval_freq 500 \\\n",
        "    --dataset_name \"./data/ablation_data/em_es_ia_sr_re\" \\\n",
        "    --lora_target_modules \"q_proj,v_proj,o_proj,k_proj,gate_proj,up_proj,down_proj\" \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --seq_length 2048 \\\n",
        "    --batch_size 8 \\\n",
        "    --run_name \"supervised_llama2_cot\" \\\n",
        "    --merge_lora\n",
        "\n",
        "print(f\"Training finished. Checkpoint should be saved in {OUTPUT_DIR}/final_checkpoint-merged\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5vGUgjwmjxK",
        "outputId": "a542f03c-7044-4672-a08d-17e340e2ff2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on meta-llama/Llama-2-7b-chat-hf. Checkpoint will be saved to ./checkpoints/cot/supervised_llama2_cot/final_checkpoint-merged\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 4, in <module>\n",
            "    from accelerate.commands.accelerate_cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
            "    from accelerate.commands.estimate import estimate_command_parser\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/estimate.py\", line 35, in <module>\n",
            "    import timm\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/__init__.py\", line 2, in <module>\n",
            "    from .layers import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/layers/__init__.py\", line 1, in <module>\n",
            "    from ._fx import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/layers/_fx.py\", line 8, in <module>\n",
            "    from torchvision.models.feature_extraction import create_feature_extractor as _create_feature_extractor\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
            "    @torch.library.register_fake(\"torchvision::nms\")\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
            "Training finished. Checkpoint should be saved in ./checkpoints/cot/supervised_llama2_cot/final_checkpoint-merged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Direct ESConv Inference (FIXED Syntax Error and Execution)\n",
        "import os\n",
        "\n",
        "# --- 1. Define the Script Content (Corrected String Formatting) ---\n",
        "INFERENCE_PY_CONTENT = \"\"\"\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "import re\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(f\"cuda:{args.gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    print(f\"Loading model from {args.model_path}...\")\n",
        "    model = LlamaForCausalLM.from_pretrained(args.model_path, device_map=device, low_cpu_mem_usage=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
        "\n",
        "    if args.json_path:\n",
        "        with open(args.json_path, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        if not data or 'dialog' not in data[0]:\n",
        "            print(\"Error: Test JSON file is not in expected ESConv format (missing 'dialog').\")\n",
        "            return\n",
        "\n",
        "        # The test.json is in the ESCoT format, where each entry is a single turn prediction.\n",
        "        # For direct inference on raw data, we need to extract the full dialog.\n",
        "        # Since the ESCoT test.json is already turn-split, we'll use the 'dialog' from the first entry's original_data,\n",
        "        # which represents the history up to the predicted turn.\n",
        "\n",
        "        # We will use the 'dialog' array from the first entry of the ESCoT test file\n",
        "        # as the context for the next turn generation.\n",
        "        if 'original_data' in data[0] and 'dialog' in data[0]['original_data']:\n",
        "             test_dialogue = data[0]['original_data']['dialog']\n",
        "        else:\n",
        "             # Fallback: Use the 'dialog' from the raw ESConv format if the file isn't pre-processed ESCoT\n",
        "             test_dialogue = data[0]['dialog']\n",
        "\n",
        "    else:\n",
        "        print(\"Error: No input dialogue provided via --json_path.\")\n",
        "        return\n",
        "\n",
        "    # --- Find the conversation history up to the turn the model needs to respond to (last seeker turn) ---\n",
        "    # The dialogue from the ESCoT test file is already truncated to the point of generation.\n",
        "    context_for_model = test_dialogue\n",
        "\n",
        "    if not context_for_model:\n",
        "        print(\"Error: Context is empty.\")\n",
        "        return\n",
        "\n",
        "    # --- Format the prompt ---\n",
        "    full_dialog_history = \"\\\\n\".join([f\"{t['speaker'].capitalize()}: {t['content'].strip()}\" for t in context_for_model])\n",
        "\n",
        "    instruction = \"Please provide the next Supporter response, including the Chain-of-Thought steps: emotion, emotion_stimuli, individual_appraisal, recognized_strategy, and strategy_reason, followed by the response.\"\n",
        "\n",
        "    # Corrected string concatenation to avoid SyntaxError\n",
        "    prompt = (\n",
        "        \"<s>Human:\\n\"\n",
        "        \"The following is an emotional support conversation. Respond as the Supporter to the final Seeker turn.\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"{full_dialog_history}\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"{instruction}\\n\"\n",
        "        \"</s><s>Assistant:\"\n",
        "    )\n",
        "\n",
        "    print(\"\\\\n--- GENERATION PROMPT ---\")\n",
        "    print(prompt)\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    print(\"\\\\nGenerating response...\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        top_k=30,\n",
        "        top_p=0.85,\n",
        "        temperature=0.5,\n",
        "        repetition_penalty=1.,\n",
        "        eos_token_id=2,\n",
        "        bos_token_id=1,\n",
        "        pad_token_id=0\n",
        "    )\n",
        "\n",
        "    rets = tokenizer.batch_decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "\n",
        "    full_output = rets[0].strip()\n",
        "\n",
        "    # Remove the prompt from the full output to get just the model's generation\n",
        "    response = full_output.replace(prompt, \"\").strip()\n",
        "    response = response.replace(\"</s>\", \"\").strip()\n",
        "\n",
        "    print(\"\\\\n--- ESCoT GENERATION ---\")\n",
        "    print(response)\n",
        "    print(\"------------------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to the ESCoT fine-tuned checkpoint.\")\n",
        "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"The id of the GPU to be used.\")\n",
        "    parser.add_argument(\"--json_path\", type=str, default=\"./data/test.json\", help=\"Path to the test JSON file containing ESConv data.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./ESCOT_DIRECT_INFERENCE\", help=\"Directory for output.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "    main(args)\n",
        "\"\"\"\n",
        "with open(\"direct_esconv_inference.py\", \"w\") as f:\n",
        "    f.write(INFERENCE_PY_CONTENT)\n",
        "print(\"Created/Updated direct_esconv_inference.py successfully.\")\n",
        "\n",
        "# --- 2. Run the Inference on the first conversation in the test set ---\n",
        "\n",
        "OUTPUT_DIR = \"./checkpoints/cot/supervised_llama2_cot\"\n",
        "MODEL_PATH = f\"{OUTPUT_DIR}/final_checkpoint-merged\"\n",
        "JSON_PATH = \"./data/test.json\"\n",
        "\n",
        "print(f\"\\nRunning direct ESConv inference using model from {MODEL_PATH} on the first dialogue in {JSON_PATH}...\")\n",
        "\n",
        "!python direct_esconv_inference.py \\\n",
        "    --model_path {MODEL_PATH} \\\n",
        "    --gpu_id 0 \\\n",
        "    --json_path {JSON_PATH}\n",
        "\n",
        "print(\"\\nDirect inference complete. Check the output above for the generated response and CoT steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb0xb_WT7KOT",
        "outputId": "e31b9860-8c4c-46fe-ab7a-3d29704859e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created/Updated direct_esconv_inference.py successfully.\n",
            "\n",
            "Running direct ESConv inference using model from ./checkpoints/cot/supervised_llama2_cot/final_checkpoint-merged on the first dialogue in ./data/test.json...\n",
            "  File \"/content/ESCoT/direct_esconv_inference.py\", line 55\n",
            "    \"<s>Human:\n",
            "    ^\n",
            "SyntaxError: unterminated string literal (detected at line 55)\n",
            "\n",
            "Direct inference complete. Check the output above for the generated response and CoT steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Upgrading transformers and accelerate libraries to resolve LlamaForCausalLM import error...\")\n",
        "!pip install --upgrade transformers accelerate bitsandbytes peft trl\n",
        "print(\"Installation complete. Restarting kernel (if in Colab) is recommended, then rerun this cell.\")\n",
        "\n",
        "# If you are not in a Colab environment or if the previous command didn't fix it,\n",
        "# you may need to run the ESCoT requirements installation again:\n",
        "# !pip install -r ESCoT/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvyJm4lr84iE",
        "outputId": "75a4ba67-8d58-4a03-d6d2-e82d499dd93b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upgrading transformers and accelerate libraries to resolve LlamaForCausalLM import error...\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.3.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Installation complete. Restarting kernel (if in Colab) is recommended, then rerun this cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# --- 1. Define Model and Data ---\n",
        "OUTPUT_DIR = \"./checkpoints/cot/supervised_llama2_cot\"\n",
        "MODEL_PATH = f\"{OUTPUT_DIR}/final_checkpoint-merged\"\n",
        "\n",
        "# The RAW ESConv dialogue data you provided, formatted as a dictionary\n",
        "RAW_ESCONV_DATA = {\n",
        "    \"dialog\": [\n",
        "        {\"speaker\": \"seeker\", \"content\": \"Hello\\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"Hello, what would you like to talk about?\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"I am having a lot of anxiety about quitting my current job. It is too stressful but pays well\\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"What makes your job stressful for you?\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"I have to deal with many people in hard financial situations and it is upsetting \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"Do you help your clients to make it to a better financial situation?\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"I do, but often they are not going to get back to what they want. Many people are going to lose their home when safeguards are lifted \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"But you offer them a better future than what they have currently. It may not be what they wanted, but it helps them in the long run.\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"That is true but sometimes I feel like I should put my feelings and health first \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"I can understand that. \"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"Is there another job that would pay you close to what you currently make?\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"Probably not. I was with the same company for a long time and I consistently get a bonus every year \"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"Is it possible to reframe how you look at your clients' dire financial situations?\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"I could try. It mostly gets to me at the end of the day \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"Some people can't do what you do because they don't have the heart to give someone else bad news. The reality is though, someone needs to fill that role and you do help people\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"That is also true. Sometimes I wonder if it really is for me though \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"I've had to deal with collections before when I was in bad financial condition. The person on the other line was really helpful though. She was understanding,\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"It may not be for you. I think you should think about the pros and cons of keeping your position. It might make things clearer for you. \"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"That is true. Maybe I just need to sit down and really think about it \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"I wouldn't stay if it really impacts your mental health in a negative way. Still, you may need to zoom out and see the bigger picture: that you provide a needed service and you do it compassionately\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"It really is a big decision \\n\"},\n",
        "        {\"speaker\": \"seeker\", \"content\": \"Thank you for the different perspective \\n\"},\n",
        "        {\"speaker\": \"supporter\", \"content\": \"No doubt, but you know in your heart what is right for you. \"},\n",
        "        {\"speaker\": \"seeker\", \"                content\": \"That is true. Thanks again \\n\"},\n",
        "        # The next turn (turn 25) is what we want the model to generate.\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- 2. Prompt Formatting Logic ---\n",
        "\n",
        "def create_cot_prompt(dialogue):\n",
        "    # Use all turns up to the last turn in the provided list as context.\n",
        "    # This ensures the model responds to the final Seeker message.\n",
        "\n",
        "    context_for_model = dialogue\n",
        "\n",
        "    full_dialog_history = \"\\n\".join([f\"{t['speaker'].capitalize()}: {t['content'].strip()}\" for t in context_for_model])\n",
        "\n",
        "    instruction = \"Please provide the next Supporter response, including the Chain-of-Thought steps: emotion, emotion_stimuli, individual_appraisal, recognized_strategy, and strategy_reason, followed by the response.\"\n",
        "\n",
        "    # Using the exact prompt template the model was trained on\n",
        "    prompt = (\n",
        "        \"<s>Human:\\n\"\n",
        "        \"The following is an emotional support conversation. Respond as the Supporter to the final Seeker turn.\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"{full_dialog_history}\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"{instruction}\\n\"\n",
        "        \"</s><s>Assistant:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# --- 3. Execution ---\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    print(f\"Loading model from {MODEL_PATH}...\")\n",
        "    model = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map=device, low_cpu_mem_usage=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ ERROR loading model: {e}\")\n",
        "    print(\"Please ensure your training (Cell 2) completed successfully and the merged model is saved at the path above.\")\n",
        "    raise\n",
        "\n",
        "# Create the prompt based on the provided ESConv dialogue\n",
        "prompt = create_cot_prompt(RAW_ESCONV_DATA['dialog'])\n",
        "\n",
        "print(\"\\n--- GENERATION PROMPT (Context for the Model) ---\")\n",
        "print(prompt)\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "print(\"\\nGenerating CoT and Response...\")\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.85,\n",
        "    temperature=0.5,\n",
        "    repetition_penalty=1.,\n",
        "    eos_token_id=2,\n",
        "    bos_token_id=1,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "rets = tokenizer.batch_decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "\n",
        "full_output = rets[0].strip()\n",
        "\n",
        "# Clean up the output to show only the model's generation\n",
        "response = full_output.replace(prompt, \"\").strip()\n",
        "response = response.replace(\"</s>\", \"\").strip()\n",
        "\n",
        "print(\"\\n🎉 --- ESCoT GENERATION (CoT & Response) --- 🎉\")\n",
        "print(response)\n",
        "print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "WcEDPrZq8Bfq",
        "outputId": "cc3213f9-403a-425f-e866-7aa37c12dc7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'LlamaForCausalLM' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2109741907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaForCausalLM' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 1. Define the Script Content for BULK Inference ---\n",
        "BULK_INFERENCE_PY_CONTENT = \"\"\"\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to create the CoT prompt from a single raw dialogue\n",
        "def create_cot_prompt(dialogue):\n",
        "    # We respond to the last turn in the provided dialogue, which should be the last Seeker turn.\n",
        "    context_for_model = dialogue\n",
        "\n",
        "    # Format the dialogue history\n",
        "    full_dialog_history = \"\\\\n\".join([f\"{t['speaker'].capitalize()}: {t['content'].strip()}\" for t in context_for_model])\n",
        "\n",
        "    instruction = \"Please provide the next Supporter response, including the Chain-of-Thought steps: emotion, emotion_stimuli, individual_appraisal, recognized_strategy, and strategy_reason, followed by the response.\"\n",
        "\n",
        "    # The prompt template must match the training format exactly.\n",
        "    prompt = (\n",
        "        \"<s>Human:\\\\n\"\n",
        "        \"The following is an emotional support conversation. Respond as the Supporter to the final Seeker turn.\\\\n\"\n",
        "        \"---\\\\n\"\n",
        "        f\"{full_dialog_history}\\\\n\"\n",
        "        \"---\\\\n\"\n",
        "        f\"{instruction}\\\\n\"\n",
        "        \"</s><s>Assistant:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(f\"cuda:{args.gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    print(f\"Loading model from {args.model_path}...\")\n",
        "    model = LlamaForCausalLM.from_pretrained(args.model_path, device_map=device, low_cpu_mem_usage=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
        "\n",
        "    # --- Load the full JSON file ---\n",
        "    try:\n",
        "        with open(args.json_path, 'r', encoding='utf-8') as file:\n",
        "            full_data = json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\\\n❌ ERROR: Input file not found at {args.json_path}\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"\\\\n❌ ERROR: Could not parse JSON file at {args.json_path}\")\n",
        "        return\n",
        "\n",
        "    # Assuming the full JSON file is a list of dictionaries, where each dictionary\n",
        "    # contains the 'dialog' key (the raw ESConv format you provided).\n",
        "    if not isinstance(full_data, list):\n",
        "        print(\"\\\\n❌ ERROR: JSON file content is not a list of dialogues.\")\n",
        "        return\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for i, conversation in enumerate(tqdm(full_data, desc=\"Generating CoT for Dialogues\")):\n",
        "        if 'dialog' not in conversation or not conversation['dialog']:\n",
        "            print(f\"Skipping entry {i}: missing or empty 'dialog' key.\")\n",
        "            continue\n",
        "\n",
        "        dialogue = conversation['dialog']\n",
        "\n",
        "        # We need the conversation history up to the last Seeker turn.\n",
        "        # We find the last Seeker turn to set the context for the model's response.\n",
        "        last_seeker_index = -1\n",
        "        for j in range(len(dialogue) - 1, -1, -1):\n",
        "            if dialogue[j]['speaker'] == 'seeker':\n",
        "                last_seeker_index = j\n",
        "                break\n",
        "\n",
        "        if last_seeker_index == -1:\n",
        "            # If the conversation is empty or starts with supporter, we skip it\n",
        "            # or use the full dialogue if the last turn is supporter (if you want to predict the next turn after supporter)\n",
        "            # For robustness, we skip if no seeker turn is available to respond to.\n",
        "            continue\n",
        "\n",
        "        context_for_generation = dialogue[:last_seeker_index + 1]\n",
        "\n",
        "        prompt = create_cot_prompt(context_for_generation)\n",
        "\n",
        "        # Generate response\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=500,\n",
        "            do_sample=True,\n",
        "            top_k=30,\n",
        "            top_p=0.85,\n",
        "            temperature=0.5,\n",
        "            repetition_penalty=1.,\n",
        "            eos_token_id=2,\n",
        "            bos_token_id=1,\n",
        "            pad_token_id=0\n",
        "        )\n",
        "\n",
        "        rets = tokenizer.batch_decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "        full_output = rets[0].strip()\n",
        "\n",
        "        response = full_output.replace(prompt, \"\").strip()\n",
        "        response = response.replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        # Save results, linking back to the original dialogue's info\n",
        "        result_entry = {\n",
        "            \"original_dialogue_id\": conversation.get(\"id\", i), # Use ID if available, otherwise index\n",
        "            \"context_for_model\": context_for_generation,\n",
        "            \"generated_cot_and_response\": response,\n",
        "            \"original_metadata\": {\n",
        "                k: v for k, v in conversation.items() if k != 'dialog'\n",
        "            }\n",
        "        }\n",
        "        all_results.append(result_entry)\n",
        "\n",
        "    # --- Save all results ---\n",
        "    output_filepath = os.path.join(args.output_dir, 'raw_esconv_cot_generations.json')\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "    with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(all_results, outfile, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\\\n\\\\n🎉 All {len(all_results)} CoT responses saved to: {output_filepath}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to the ESCoT fine-tuned checkpoint.\")\n",
        "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"The id of the GPU to be used.\")\n",
        "    parser.add_argument(\"--json_path\", type=str, required=True, help=\"Path to the full raw ESConv JSON file.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./ESCOT_RAW_DATA_RESULTS\", help=\"Directory for output.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n",
        "\"\"\"\n",
        "with open(\"bulk_raw_esconv_inference.py\", \"w\") as f:\n",
        "    f.write(BULK_INFERENCE_PY_CONTENT)\n",
        "print(\"Created bulk_raw_esconv_inference.py successfully.\")\n",
        "\n",
        "# --- 2. Run the BULK Inference ---\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_DIR = \"./checkpoints/cot/supervised_llama2_cot\"\n",
        "MODEL_PATH = f\"{OUTPUT_DIR}/final_checkpoint-merged\"\n",
        "\n",
        "# ⚠️ YOU MUST REPLACE THIS PLACEHOLDER with the actual name of your file ⚠️\n",
        "RAW_ESCONV_FILE = \"RAW_ESCONV_DATASET.json\"\n",
        "\n",
        "print(f\"\\nStarting bulk CoT generation using model from {MODEL_PATH} on file: {RAW_ESCONV_FILE}...\")\n",
        "\n",
        "!python bulk_raw_esconv_inference.py \\\n",
        "    --model_path {MODEL_PATH} \\\n",
        "    --gpu_id 0 \\\n",
        "    --json_path {RAW_ESCONV_FILE}\n",
        "\n",
        "print(\"\\nBulk inference process complete. Check the directory ./ESCOT_RAW_DATA_RESULTS for the final JSON file.\")"
      ],
      "metadata": {
        "id": "wbJTjMo58wMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}