# -*- coding: utf-8 -*-
"""llama 2 weights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RiusrBdKcE7LKUqcFJuDW4FWrImSki3T
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/TeigenZhang/ESCoT.git
# %cd ESCoT

!pip install -q -r requirements.txt bitsandbytes accelerate transformers peft tqdm

!mkdir -p checkpoints/cot/supervised_llama2_cot/final_checkpoint

!wget https://huggingface.co/TeigenZhang/ESCoT-Llama2-7b-CoT/resolve/main/adapter_model.bin -O checkpoints/cot/supervised_llama2_cot/final_checkpoint/adapter_model.bin
!wget https://huggingface.co/TeigenZhang/ESCoT-Llama2-7b-CoT/resolve/main/adapter_config.json -O checkpoints/cot/supervised_llama2_cot/final_checkpoint/adapter_config.json

!mkdir -p data/ablation_data/em_es_ia_sr_re

!wget https://huggingface.co/datasets/TeigenZhang/ESConv/resolve/main/empathetic_dialogue_test.json -O data/ablation_data/em_es_ia_sr_re/empathetic_dialogue_test.json

!pip install -q transformers peft accelerate bitsandbytes

!pip install -q huggingface_hub

from huggingface_hub import login

# Replace with your actual Hugging Face token
login("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")

from huggingface_hub import whoami
print(whoami())

from transformers import LlamaForCausalLM, LlamaTokenizer
from peft import PeftModel
import torch

base_model = "meta-llama/Llama-2-7b-chat-hf"
adapter_path = "./checkpoints/cot/supervised_llama2_cot/final_checkpoint"

tokenizer = LlamaTokenizer.from_pretrained(base_model)
model = LlamaForCausalLM.from_pretrained(base_model, device_map="auto", torch_dtype=torch.float16)
model = PeftModel.from_pretrained(model, adapter_path)
model.eval()

print("✅ Model + Adapter loaded successfully!")

from huggingface_hub import login

# Replace 'YOUR_HF_TOKEN' with your Hugging Face token
login(token="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")

from huggingface_hub import hf_hub_download
import os

# Your Hugging Face token
HF_TOKEN = "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"

# Target directory
base_dir = "./pretrained_models/Llama-2-7b-chat-hf"
os.makedirs(base_dir, exist_ok=True)

# Files to download from Llama2 repo
files = [
    "config.json",
    "tokenizer.model",
    "tokenizer_config.json",
    "generation_config.json",
    "pytorch_model-00001-of-00002.bin",
    "pytorch_model-00002-of-00002.bin"
]

for f in files:
    hf_hub_download(
        repo_id="meta-llama/Llama-2-7b-chat-hf",
        filename=f,
        cache_dir=base_dir,
        use_auth_token=HF_TOKEN
    )

print("All files downloaded to", base_dir)

!apt-get install git-lfs -y
!git lfs install

!ls ./checkpoints/cot/supervised_llama2_cot/final_checkpoint

from google.colab import files
uploaded = files.upload()  # upload your esconv.json

import json
import os

# --- paths ---
input_path = "ESConv.json"   # uploaded file name
output_dir = "./data/ablation_data/em_es_ia_sr_re"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "esconv_test.json")

# --- load ESConv data ---
with open(input_path, "r", encoding="utf-8") as f:
    esconv_data = json.load(f)

converted_data = []

# --- convert each dialogue to ESCoT-like format ---
for idx, sample in enumerate(esconv_data):
    dialog_history = []
    for turn in sample["dialog"]:
        dialog_history.append({
            "role": turn["speaker"],
            "content": turn["content"].strip()
        })

    # Build ESCoT-style record
    converted_entry = {
        "id": f"esconv_{idx}",
        "experience_type": sample.get("experience_type", ""),
        "emotion_type": sample.get("emotion_type", ""),
        "problem_type": sample.get("problem_type", ""),
        "situation": sample.get("situation", ""),
        "dialog_history": dialog_history,
        "survey_score": sample.get("survey_score", {}),
        "meta": {
            "dataset": "ESConv"
        }
    }
    converted_data.append(converted_entry)

# --- save converted file ---
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(converted_data, f, indent=2, ensure_ascii=False)

print(f"✅ Converted {len(converted_data)} samples saved to: {output_path}")