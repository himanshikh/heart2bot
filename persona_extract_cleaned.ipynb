{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (4.56.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/himanshikhanna/Desktop/jupiter/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.56.2\n",
      "    Uninstalling transformers-4.56.2:\n",
      "      Successfully uninstalled transformers-4.56.2\n",
      "Successfully installed transformers-4.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Persona extractor tailored for Personachat (Persona + chat) and ESConv (dialog -> content)\n",
    "\n",
    "import os, json, torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "persona_file = \"/content/personachat.json\"   # your PersonaChat (you provided sample)\n",
    "esconv_file  = \"/content/ESConv.json\"       # ESConv (you provided sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PersonaChat -> history -> persona\n",
    "\n",
    "def load_personachat_history2persona(path, max_examples=None):\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    examples = []\n",
    "    for item in data:\n",
    "        # Persona text: you have \"Persona\" key in your sample\n",
    "        persona_text = \"\"\n",
    "        # common keys in your sample: \"Persona\"\n",
    "        for k in [\"Persona\", \"persona\", \"personality\"]:\n",
    "            if k in item and item[k]:\n",
    "                val = item[k]\n",
    "                if isinstance(val, list):\n",
    "                    persona_text = \" | \".join([v.strip() for v in val if isinstance(v, str) and v.strip()])\n",
    "                else:\n",
    "                    persona_text = str(val).strip()\n",
    "                break\n",
    "\n",
    "        # chat: your sample has \"chat\" as newline-separated string\n",
    "        chat = item.get(\"chat\") or item.get(\"dialog\") or item.get(\"dialogue\") or item.get(\"utterances\") or \"\"\n",
    "        turns = []\n",
    "        if isinstance(chat, str):\n",
    "            turns = [t.strip() for t in chat.split(\"\\n\") if t.strip()]\n",
    "        elif isinstance(chat, list):\n",
    "            for u in chat:\n",
    "                if isinstance(u, str):\n",
    "                    turns.append(u.strip())\n",
    "                elif isinstance(u, dict):\n",
    "                    txt = u.get(\"content\") or u.get(\"text\") or u.get(\"utterance\") or \"\"\n",
    "                    if txt:\n",
    "                        turns.append(str(txt).strip())\n",
    "\n",
    "        if not persona_text or not turns:\n",
    "            continue\n",
    "\n",
    "        src = \"history: \" + \" </s> \".join(turns)\n",
    "        tgt = persona_text\n",
    "        examples.append({\"source\": src, \"target\": tgt})\n",
    "        if max_examples and len(examples) >= max_examples:\n",
    "            break\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(examples)} history->persona examples from {path}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/personachat.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Quick check on PersonaChat loader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m persona_examples \u001b[38;5;241m=\u001b[39m load_personachat_history2persona(persona_file, max_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(persona_examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: No history->persona examples found. Check your personachat.json keys (Persona/chat).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mload_personachat_history2persona\u001b[0;34m(path, max_examples)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_personachat_history2persona\u001b[39m(path, max_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m     examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Persona text: you have \"Persona\" key in your sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/jupiter/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/personachat.json'"
     ]
    }
   ],
   "source": [
    "# Quick check on PersonaChat loader\n",
    "persona_examples = load_personachat_history2persona(persona_file, max_examples=20000)\n",
    "if len(persona_examples) == 0:\n",
    "    raise SystemExit(\"ERROR: No history->persona examples found. Check your personachat.json keys (Persona/chat).\")\n",
    "\n",
    "print(\"\\n=== SAMPLE TRAINING PAIRS (first 5) ===\")\n",
    "for i, ex in enumerate(persona_examples[:5]):\n",
    "    print(f\"\\n[{i+1}] SOURCE (trunc): {ex['source'][:200]}\")\n",
    "    print(f\"[{i+1}] TARGET (persona): {ex['target'][:200]}\")\n",
    "\n",
    "# Trim for quick runs (optional)\n",
    "persona_examples = persona_examples[:15000]\n",
    "n = len(persona_examples)\n",
    "train_ds = Dataset.from_list(persona_examples[:int(0.9*n)])\n",
    "val_ds   = Dataset.from_list(persona_examples[int(0.9*n):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3e7cb1a73a4fe6afec92946430abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef343995ae93440ca34144bc3ee92f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b2924375c64707bed416917143ac14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38efee6b77ad458593e81a7b5a3981c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6616df0e794b0fb09657097ef16c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddb9f8297124700b872a299676ef88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOKENIZED LABELS SAMPLE ===\n",
      "Example 1: label tokens len=30 preview=[0, 118, 101, 7, 21054, 523, 1611, 4, 939, 101, 7, 213, 8217, 4, 939, 101, 7, 4511, 10, 7323]\n",
      "Example 2: label tokens len=29 preview=[0, 4783, 3795, 16, 127, 275, 1441, 4, 939, 33, 237, 7502, 4, 939, 679, 14, 9374, 16355, 29, 32]\n",
      "Example 3: label tokens len=46 preview=[0, 118, 56, 10, 10196, 23, 400, 7364, 94, 363, 4, 939, 173, 25, 10, 1413, 62, 10688, 4, 939]\n",
      "Example 4: label tokens len=25 preview=[0, 118, 524, 182, 7714, 4, 939, 3568, 9872, 4, 939, 33, 6219, 2549, 4, 939, 657, 44821, 21050, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f4cbbcf20c4e3c9190f677c620a420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cc0ddbaf1749dcaa839f4753417843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model & Tokenization\n",
    "\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    model_inputs = tokenizer(batch[\"source\"], truncation=True, max_length=512)\n",
    "    # correct seq2seq target tokenization\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"target\"], truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Inspect tokenized labels for a small sample to detect -100 or empty targets\n",
    "sample_check = Dataset.from_list(persona_examples[:4])\n",
    "sample_tok = sample_check.map(tokenize_fn, batched=True, remove_columns=sample_check.column_names)\n",
    "print(\"\\n=== TOKENIZED LABELS SAMPLE ===\")\n",
    "for i, item in enumerate(sample_tok):\n",
    "    labels = item[\"labels\"]\n",
    "    print(f\"Example {i+1}: label tokens len={len(labels)} preview={labels[:20]}\")\n",
    "    if len(labels) == 0 or all([lbl == -100 for lbl in labels]):\n",
    "        raise SystemExit(\"ERROR: tokenized labels appear empty or all -100. That means target tokenization failed or target is empty.\")\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tokenized   = val_ds.map(tokenize_fn,   batched=True, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SANITY GENERATE (pre-train) on first source ===\n",
      "SOURCE (trunc): history: hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape . </s> you must be very fast . hunting is one of my favorite hobbies . </s> i am ! for my hobby i like \n",
      "MODEL OUTPUT (pre-train): 'history: hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .'\n"
     ]
    }
   ],
   "source": [
    "# Sanity generation BEFORE training (should produce something but likely not persona-yet)\n",
    "\n",
    "def generate_from_text(inp_text, max_len=128):\n",
    "    inputs = tokenizer(inp_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inputs, max_length=max_len, num_beams=4, early_stopping=True, min_length=1)\n",
    "    return tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"\\n=== SANITY GENERATE (pre-train) on first source ===\")\n",
    "test_src = persona_examples[0][\"source\"]\n",
    "print(\"SOURCE (trunc):\", test_src[:200])\n",
    "print(\"MODEL OUTPUT (pre-train):\", repr(generate_from_text(test_src)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1640714973.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START TRAINING (1 epoch quick) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2012' max='2012' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2012/2012 08:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.566100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "=== SANITY GENERATE (post-train) on first source ===\n",
      "MODEL OUTPUT (post-train): 'i like to remodel homes. my favorite food is meat. i like to hunt. i am in high school.'\n"
     ]
    }
   ],
   "source": [
    "# Training (light)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./persona_bart\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=200,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n=== START TRAINING (1 epoch quick) ===\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./persona_bart\")\n",
    "tokenizer.save_pretrained(\"./persona_bart\")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "print(\"\\n=== SANITY GENERATE (post-train) on first source ===\")\n",
    "print(\"MODEL OUTPUT (post-train):\", repr(generate_from_text(test_src)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas:   6%|â–Œ         | 74/1300 [12:00<4:19:23, 12.69s/it]"
     ]
    }
   ],
   "source": [
    "# ESConv inference - tailored to your ESConv sample format\n",
    "\n",
    "def load_esconv(path):\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    if isinstance(data, dict):\n",
    "        # repository sometimes wraps dialogs in \"dialogs\" key\n",
    "        if \"dialogs\" in data:\n",
    "            return data[\"dialogs\"]\n",
    "        for v in data.values():\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        return []\n",
    "    return data\n",
    "\n",
    "def extract_texts_from_entry(entry):\n",
    "    # Your ESConv sample uses key \"dialog\" which is a list of dicts with \"content\"\n",
    "    texts = []\n",
    "    if isinstance(entry, list):\n",
    "        for u in entry:\n",
    "            if isinstance(u, str):\n",
    "                texts.append(u.strip())\n",
    "            elif isinstance(u, dict):\n",
    "                txt = u.get(\"content\") or u.get(\"text\") or u.get(\"utterance\") or \"\"\n",
    "                if txt:\n",
    "                    texts.append(str(txt).strip())\n",
    "    elif isinstance(entry, dict):\n",
    "        # check common keys including \"dialog\"\n",
    "        for k in [\"dialog\", \"dialogue\", \"utterances\", \"conversation\", \"turns\", \"history\"]:\n",
    "            if k in entry and entry[k]:\n",
    "                return extract_texts_from_entry(entry[k])\n",
    "        # fallback: if entry has speaker/text pairs, try to extract them\n",
    "        if \"content\" in entry:\n",
    "            texts.append(str(entry[\"content\"]).strip())\n",
    "    return texts\n",
    "\n",
    "def compose_input(history_texts):\n",
    "    return \"history: \" + \" </s> \".join(history_texts)\n",
    "\n",
    "def infer_persona_from_dialogue(dialogue):\n",
    "    utterances = extract_texts_from_entry(dialogue)\n",
    "    if not utterances:\n",
    "        return \"\", []\n",
    "    history = [u for u in utterances if u]\n",
    "    if not history:\n",
    "        return \"\", []\n",
    "    inp = compose_input(history)\n",
    "    inputs = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True, min_length=3)\n",
    "    persona = tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # persona_list generation (turnwise) - optional/useful\n",
    "    persona_list = []\n",
    "    for i in range(3, len(history)):\n",
    "        inp2 = compose_input(history[:i+1])\n",
    "        inputs2 = tokenizer(inp2, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            ids2 = model.generate(**inputs2, max_length=128, num_beams=4, early_stopping=True, min_length=3)\n",
    "        persona_list.append(tokenizer.decode(ids2[0], skip_special_tokens=True).strip())\n",
    "    return persona, persona_list\n",
    "\n",
    "esconv_data = load_esconv(esconv_file)\n",
    "pesconv_output = []\n",
    "for dlg in tqdm(esconv_data, desc=\"Generating personas\"):\n",
    "    persona, plist = infer_persona_from_dialogue(dlg)\n",
    "    newdlg = dlg.copy() if isinstance(dlg, dict) else {\"dialog\": dlg}\n",
    "    newdlg[\"persona\"] = persona\n",
    "    newdlg[\"persona_list\"] = plist\n",
    "    pesconv_output.append(newdlg)\n",
    "\n",
    "with open(\"PESConv.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pesconv_output, f, ensure_ascii=False, indent=2)\n",
    "print(\"PESConv.json saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few samples\n",
    "print(\"\\n EXAMPLE OUTPUTS \")\n",
    "for i, d in enumerate(pesconv_output[:3]):\n",
    "    print(f\"\\nDialog {i+1}:\")\n",
    "    # show first few utterances\n",
    "    utterances = extract_texts_from_entry(d)\n",
    "    print(\"Utterances (few):\", utterances[:4])\n",
    "    print(\"Predicted Persona:\", repr(d.get(\"persona\",\"\")))\n",
    "    print(\"Persona list (few):\", d.get(\"persona_list\", [])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact Memory Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class FactMemory:\n",
    "    def __init__(self, model_path=\"./persona_bart\", memory_path=\"user_memory.json\"):\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.memory_path = memory_path\n",
    "\n",
    "        # Load or initialize memory\n",
    "        if os.path.exists(memory_path):\n",
    "            self.memory = json.load(open(memory_path, \"r\", encoding=\"utf-8\"))\n",
    "        else:\n",
    "            self.memory = {\n",
    "                \"persona_facts\": [],\n",
    "                \"session_history\": [],\n",
    "                \"summaries\": []\n",
    "            }\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.memory_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.memory, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Add new persona facts (from extraction)\n",
    "    def add_persona_facts(self, facts):\n",
    "        for f in facts:\n",
    "            if f not in self.memory[\"persona_facts\"]:\n",
    "                self.memory[\"persona_facts\"].append(f)\n",
    "        self.save()\n",
    "\n",
    "    # Record ongoing conversation\n",
    "    def add_session_turn(self, user, bot):\n",
    "        self.memory[\"session_history\"].append({\"user\": user, \"bot\": bot})\n",
    "        self.save()\n",
    "\n",
    "    # Retrieve all memory for dialogue conditioning\n",
    "    def get_context(self):\n",
    "        persona = \" \".join(self.memory[\"persona_facts\"])\n",
    "        session = \" \".join([f\"User: {x['user']} Bot: {x['bot']}\" for x in self.memory[\"session_history\"][-5:]])\n",
    "        return f\"Persona: {persona}\\nHistory: {session}\"\n",
    "\n",
    "    # Summarize long history (to keep memory concise)\n",
    "    def summarize_history(self, max_len=10):\n",
    "        if len(self.memory[\"session_history\"]) > max_len:\n",
    "            text = \" \".join([f\"User: {x['user']} Bot: {x['bot']}\" for x in self.memory[\"session_history\"]])\n",
    "            summary = self.summarize_text(text)\n",
    "            self.memory[\"summaries\"].append(summary)\n",
    "            self.memory[\"session_history\"] = []\n",
    "            self.save()\n",
    "            return summary\n",
    "        return None\n",
    "\n",
    "    # Uses model to summarize session history\n",
    "    def summarize_text(self, text):\n",
    "        prompt = f\"Summarize key user facts and themes: {text}\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        ids = self.model.generate(**inputs, max_length=100, num_beams=4)\n",
    "        return self.tokenizer.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate Memory with Dialogue Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, fact_memory):\n",
    "    # Update session history with user input\n",
    "    fact_memory.add_session_turn(user=user_input, bot=\"\")\n",
    "\n",
    "    # Prepare context (persona + history)\n",
    "    context = fact_memory.get_context()\n",
    "    prompt = f\"{context}\\nUser: {user_input}\\nBot:\"\n",
    "\n",
    "    inputs = fact_memory.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    ids = fact_memory.model.generate(**inputs, max_length=80, num_beams=4)\n",
    "    reply = fact_memory.tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Update memory with generated bot response\n",
    "    fact_memory.memory[\"session_history\"][-1][\"bot\"] = reply\n",
    "    fact_memory.save()\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Controlled Updates (View, Add, Delete Facts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_memory(fact_memory):\n",
    "    print(\"=== Persona Facts ===\")\n",
    "    for i, f in enumerate(fact_memory.memory[\"persona_facts\"]):\n",
    "        print(f\"{i+1}. {f}\")\n",
    "    print(\"\\n=== Summaries ===\")\n",
    "    for s in fact_memory.memory[\"summaries\"]:\n",
    "        print(\"-\", s)\n",
    "\n",
    "def add_fact(fact_memory, new_fact):\n",
    "    fact_memory.add_persona_facts([new_fact])\n",
    "    print(f\"Added fact: '{new_fact}'\")\n",
    "\n",
    "def delete_fact(fact_memory, index):\n",
    "    try:\n",
    "        removed = fact_memory.memory[\"persona_facts\"].pop(index - 1)\n",
    "        fact_memory.save()\n",
    "        print(f\"Deleted fact: '{removed}'\")\n",
    "    except IndexError:\n",
    "        print(\"Invalid index.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
